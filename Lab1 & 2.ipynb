{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/eugenia_bogacheva/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/eugenia_bogacheva/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package sentiwordnet to\n",
      "[nltk_data]     /Users/eugenia_bogacheva/nltk_data...\n",
      "[nltk_data]   Package sentiwordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/eugenia_bogacheva/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import preprocessor as pr\n",
    "import pandas as pd\n",
    "from langdetect import detect\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, f1_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from nltk.corpus import wordnet\n",
    "nltk.download('wordnet')\n",
    "nltk.download('sentiwordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import word_tokenize\n",
    "from nltk.sentiment.util import mark_negation\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PREPROCESSING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_topics = []\n",
    "test_sentiment = []\n",
    "test_text = []\n",
    "with open('test.csv') as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    for row in reader:\n",
    "        test_topics.append(row[\"Topic\"])\n",
    "        test_sentiment.append(row[\"Sentiment\"])\n",
    "        test_text.append(row[\"TweetText\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_topics = []\n",
    "train_sentiment = []\n",
    "train_text = []\n",
    "with open('train.csv') as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    for row in reader:\n",
    "        train_topics.append(row[\"Topic\"])\n",
    "        train_sentiment.append(row[\"Sentiment\"])\n",
    "        train_text.append(row[\"TweetText\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples of tweet text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now all @Apple has to do is get swype on the iphone and it will be crack. Iphone that is\n",
      "RT @katebetts: Another great James Stewart story in today's NY Times about importance of architecture in @apple retail success http://t. ...\n",
      "Pissed with whoever designs keyboards with @apple for not having a home and end key.  working on the CLI i use those keys often\n",
      "In front of the @apple store. So many blue shirts I feel like I'm at a smurf reunion. \n",
      "iPadのビジネス活用セミナー@Appleストア銀座なう。イシン（株）の高木さんを見にきたよ。\n"
     ]
    }
   ],
   "source": [
    "for i in [0,111,333,555,888]:\n",
    "    print(train_text[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropping irrelevant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentiment_without_irrelevant = []\n",
    "train_topics_without_irrelevant = []\n",
    "train_text_without_irrelevant = []\n",
    "\n",
    "for i in range(len(train_sentiment)):\n",
    "    if train_sentiment[i] != 'irrelevant':\n",
    "        train_sentiment_without_irrelevant.append(train_sentiment[i])\n",
    "        train_topics_without_irrelevant.append(train_topics[i])\n",
    "        train_text_without_irrelevant.append(train_text[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentiment_without_irrelevant = []\n",
    "test_topics_without_irrelevant = []\n",
    "test_text_without_irrelevant = []\n",
    "\n",
    "for i in range(len(test_sentiment)):\n",
    "    if test_sentiment[i] != 'irrelevant':\n",
    "        test_sentiment_without_irrelevant.append(test_sentiment[i])\n",
    "        test_topics_without_irrelevant.append(test_topics[i])\n",
    "        test_text_without_irrelevant.append(test_text[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(texts):\n",
    "    documents = []\n",
    "    pr.set_options(pr.OPT.URL, pr.OPT.HASHTAG, pr.OPT.MENTION)\n",
    "    stemmer = WordNetLemmatizer()\n",
    "\n",
    "    for sen in range(0, len(texts)):\n",
    "        # remove URLs, hashtags and mentions\n",
    "        document = pr.clean(texts[sen])\n",
    "\n",
    "        # Converting to Lowercase\n",
    "        document = document.lower()\n",
    "\n",
    "        # Lemmatization\n",
    "        document = document.split()\n",
    "\n",
    "        document = [stemmer.lemmatize(word) for word in document]\n",
    "        document = ' '.join(document)\n",
    "\n",
    "        documents.append(document)\n",
    "        \n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text_cleaned = preprocess(train_text)\n",
    "test_text_cleaned = preprocess(test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text_without_irrelevant_cleaned = preprocess(train_text_without_irrelevant)\n",
    "test_text_without_irrelevant_cleaned = preprocess(test_text_without_irrelevant)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AUXILARY FUNCTIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function for calculating the confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_confusion_matrix(predictions, true_labels):\n",
    "    print(confusion_matrix(predictions, true_labels, labels = ['positive', 'negative', 'neutral', 'irrelevant']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def float_to_percents(number):\n",
    "    return round(number * 100, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics(predictions, true_labels):\n",
    "    accuracy = float_to_percents(accuracy_score(predictions, true_labels))\n",
    "    f1_score_weighted = float_to_percents(f1_score(predictions, true_labels, average = 'weighted'))\n",
    "    f1_score_micro = float_to_percents(f1_score(predictions, true_labels, average = 'micro'))\n",
    "    f1_score_macro = float_to_percents(f1_score(predictions, true_labels, average = 'macro'))\n",
    "    return [accuracy, f1_score_weighted, f1_score_micro, f1_score_macro]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_metrics_df(metrics_dict):\n",
    "    df = pd.DataFrame.from_dict(metrics_dict, orient='index')\n",
    "    df = df.rename(columns={0:'Accuracy', 1: 'Weighted F1 score', 2: 'Micro F1 score', 3: 'Macro F1 score'})\n",
    "    return df.sort_values(by=['Accuracy'], ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_metrics = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizers = CountVectorizer, TfidfVectorizer\n",
    "vectorizers_names = ['CountVectorizer', 'TfidfVectorizer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = [LogisticRegression(solver='lbfgs', multi_class='multinomial'), LinearSVC(), RandomForestClassifier()]\n",
    "classifiers_names = ['LogisticRegression', 'LinearSVC', 'Random Forest']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "ngrams_models_metrics = {}\n",
    "ngram_ranges = [(1, 1), (1, 2), (2, 2)]\n",
    "max_features = [4000, 5000, 6000, 10000, 15000]\n",
    "\n",
    "for ngram_range in ngram_ranges:\n",
    "    for vct_name, vectorizer in zip(vectorizers_names, vectorizers):\n",
    "        for clf_name, classifier in zip(classifiers_names, classifiers):\n",
    "            for mf in max_features:\n",
    "                clf = Pipeline([\n",
    "                    ('vectorizer', vectorizer(analyzer=\"word\",\n",
    "                                                   ngram_range=ngram_range,\n",
    "                                                   tokenizer=word_tokenize,\n",
    "                                                   max_features=mf  ) ),\n",
    "                    ('classifier', classifier)\n",
    "                ])\n",
    "\n",
    "\n",
    "                clf.fit(train_text_cleaned, train_sentiment)\n",
    "                pred =clf.predict(test_text_cleaned)\n",
    "\n",
    "                name = 'NGR: ' + str(ngram_range) + '; V: ' + vct_name + '; CLF: ' + clf_name + '; MF: ' + str(mf) \n",
    "                ngrams_models_metrics[name] = metrics(pred, test_sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The best configurations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Weighted F1 score</th>\n",
       "      <th>Micro F1 score</th>\n",
       "      <th>Macro F1 score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>NGR: (1, 2); V: CountVectorizer; CLF: LogisticRegression; MF: 4000</th>\n",
       "      <td>78.95</td>\n",
       "      <td>80.03</td>\n",
       "      <td>78.95</td>\n",
       "      <td>69.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NGR: (1, 2); V: CountVectorizer; CLF: LogisticRegression; MF: 5000</th>\n",
       "      <td>78.95</td>\n",
       "      <td>79.98</td>\n",
       "      <td>78.95</td>\n",
       "      <td>69.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NGR: (1, 1); V: TfidfVectorizer; CLF: LinearSVC; MF: 5000</th>\n",
       "      <td>78.65</td>\n",
       "      <td>79.33</td>\n",
       "      <td>78.65</td>\n",
       "      <td>71.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NGR: (1, 2); V: CountVectorizer; CLF: LogisticRegression; MF: 6000</th>\n",
       "      <td>78.36</td>\n",
       "      <td>79.37</td>\n",
       "      <td>78.36</td>\n",
       "      <td>69.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NGR: (1, 1); V: TfidfVectorizer; CLF: LinearSVC; MF: 6000</th>\n",
       "      <td>78.07</td>\n",
       "      <td>78.80</td>\n",
       "      <td>78.07</td>\n",
       "      <td>70.60</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    Accuracy  \\\n",
       "NGR: (1, 2); V: CountVectorizer; CLF: LogisticR...     78.95   \n",
       "NGR: (1, 2); V: CountVectorizer; CLF: LogisticR...     78.95   \n",
       "NGR: (1, 1); V: TfidfVectorizer; CLF: LinearSVC...     78.65   \n",
       "NGR: (1, 2); V: CountVectorizer; CLF: LogisticR...     78.36   \n",
       "NGR: (1, 1); V: TfidfVectorizer; CLF: LinearSVC...     78.07   \n",
       "\n",
       "                                                    Weighted F1 score  \\\n",
       "NGR: (1, 2); V: CountVectorizer; CLF: LogisticR...              80.03   \n",
       "NGR: (1, 2); V: CountVectorizer; CLF: LogisticR...              79.98   \n",
       "NGR: (1, 1); V: TfidfVectorizer; CLF: LinearSVC...              79.33   \n",
       "NGR: (1, 2); V: CountVectorizer; CLF: LogisticR...              79.37   \n",
       "NGR: (1, 1); V: TfidfVectorizer; CLF: LinearSVC...              78.80   \n",
       "\n",
       "                                                    Micro F1 score  \\\n",
       "NGR: (1, 2); V: CountVectorizer; CLF: LogisticR...           78.95   \n",
       "NGR: (1, 2); V: CountVectorizer; CLF: LogisticR...           78.95   \n",
       "NGR: (1, 1); V: TfidfVectorizer; CLF: LinearSVC...           78.65   \n",
       "NGR: (1, 2); V: CountVectorizer; CLF: LogisticR...           78.36   \n",
       "NGR: (1, 1); V: TfidfVectorizer; CLF: LinearSVC...           78.07   \n",
       "\n",
       "                                                    Macro F1 score  \n",
       "NGR: (1, 2); V: CountVectorizer; CLF: LogisticR...           69.52  \n",
       "NGR: (1, 2); V: CountVectorizer; CLF: LogisticR...           69.85  \n",
       "NGR: (1, 1); V: TfidfVectorizer; CLF: LinearSVC...           71.19  \n",
       "NGR: (1, 2); V: CountVectorizer; CLF: LogisticR...           69.35  \n",
       "NGR: (1, 1); V: TfidfVectorizer; CLF: LinearSVC...           70.60  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ngrams_metrics = pd.DataFrame.from_dict(ngrams_models_metrics, orient='index')\n",
    "df_ngrams_metrics = df_ngrams_metrics.rename(columns={0:'Accuracy', 1: 'Weighted F1 score', 2: 'Micro F1 score', 3: 'Macro F1 score'})\n",
    "df_ngrams_metrics.sort_values(by=['Accuracy'], ascending=False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### And the worst ones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Weighted F1 score</th>\n",
       "      <th>Micro F1 score</th>\n",
       "      <th>Macro F1 score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>NGR: (2, 2); V: CountVectorizer; CLF: Random Forest; MF: 15000</th>\n",
       "      <td>59.36</td>\n",
       "      <td>63.47</td>\n",
       "      <td>59.36</td>\n",
       "      <td>47.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NGR: (2, 2); V: CountVectorizer; CLF: Random Forest; MF: 10000</th>\n",
       "      <td>59.36</td>\n",
       "      <td>62.52</td>\n",
       "      <td>59.36</td>\n",
       "      <td>48.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NGR: (2, 2); V: TfidfVectorizer; CLF: Random Forest; MF: 15000</th>\n",
       "      <td>59.36</td>\n",
       "      <td>62.23</td>\n",
       "      <td>59.36</td>\n",
       "      <td>52.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NGR: (2, 2); V: TfidfVectorizer; CLF: Random Forest; MF: 6000</th>\n",
       "      <td>57.60</td>\n",
       "      <td>60.55</td>\n",
       "      <td>57.60</td>\n",
       "      <td>48.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NGR: (2, 2); V: TfidfVectorizer; CLF: Random Forest; MF: 10000</th>\n",
       "      <td>56.43</td>\n",
       "      <td>59.21</td>\n",
       "      <td>56.43</td>\n",
       "      <td>46.96</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    Accuracy  \\\n",
       "NGR: (2, 2); V: CountVectorizer; CLF: Random Fo...     59.36   \n",
       "NGR: (2, 2); V: CountVectorizer; CLF: Random Fo...     59.36   \n",
       "NGR: (2, 2); V: TfidfVectorizer; CLF: Random Fo...     59.36   \n",
       "NGR: (2, 2); V: TfidfVectorizer; CLF: Random Fo...     57.60   \n",
       "NGR: (2, 2); V: TfidfVectorizer; CLF: Random Fo...     56.43   \n",
       "\n",
       "                                                    Weighted F1 score  \\\n",
       "NGR: (2, 2); V: CountVectorizer; CLF: Random Fo...              63.47   \n",
       "NGR: (2, 2); V: CountVectorizer; CLF: Random Fo...              62.52   \n",
       "NGR: (2, 2); V: TfidfVectorizer; CLF: Random Fo...              62.23   \n",
       "NGR: (2, 2); V: TfidfVectorizer; CLF: Random Fo...              60.55   \n",
       "NGR: (2, 2); V: TfidfVectorizer; CLF: Random Fo...              59.21   \n",
       "\n",
       "                                                    Micro F1 score  \\\n",
       "NGR: (2, 2); V: CountVectorizer; CLF: Random Fo...           59.36   \n",
       "NGR: (2, 2); V: CountVectorizer; CLF: Random Fo...           59.36   \n",
       "NGR: (2, 2); V: TfidfVectorizer; CLF: Random Fo...           59.36   \n",
       "NGR: (2, 2); V: TfidfVectorizer; CLF: Random Fo...           57.60   \n",
       "NGR: (2, 2); V: TfidfVectorizer; CLF: Random Fo...           56.43   \n",
       "\n",
       "                                                    Macro F1 score  \n",
       "NGR: (2, 2); V: CountVectorizer; CLF: Random Fo...           47.66  \n",
       "NGR: (2, 2); V: CountVectorizer; CLF: Random Fo...           48.75  \n",
       "NGR: (2, 2); V: TfidfVectorizer; CLF: Random Fo...           52.02  \n",
       "NGR: (2, 2); V: TfidfVectorizer; CLF: Random Fo...           48.09  \n",
       "NGR: (2, 2); V: TfidfVectorizer; CLF: Random Fo...           46.96  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ngrams_metrics.sort_values(by=['Accuracy'], ascending=False).tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_metrics['Ngrams'] = ngrams_models_metrics['NGR: (1, 2); V: CountVectorizer; CLF: LogisticRegression; MF: 4000']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## If we could drop the irrelevant ones from both train and test set..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngrams_models_metrics_without_irrelevant = {}\n",
    "clf = Pipeline([\n",
    "    ('vectorizer', TfidfVectorizer(analyzer=\"word\", ngram_range=(1,1), tokenizer=word_tokenize,\n",
    "                                                   max_features= 5000  ) ),\n",
    "                    ('classifier', LinearSVC())])\n",
    "\n",
    "clf.fit(train_text_without_irrelevant_cleaned, train_sentiment_without_irrelevant)\n",
    "pred =clf.predict(test_text_without_irrelevant_cleaned)\n",
    "ngrams_models_metrics_without_irrelevant['NGR: (1, 1); V: TfidfVectorizer; CLF: LinearSVC; MF: 5000'] = metrics(pred, test_sentiment_without_irrelevant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Weighted F1 score</th>\n",
       "      <th>Micro F1 score</th>\n",
       "      <th>Macro F1 score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>NGR: (1, 1); V: TfidfVectorizer; CLF: LinearSVC; MF: 5000</th>\n",
       "      <td>79.75</td>\n",
       "      <td>80.85</td>\n",
       "      <td>79.75</td>\n",
       "      <td>70.52</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    Accuracy  \\\n",
       "NGR: (1, 1); V: TfidfVectorizer; CLF: LinearSVC...     79.75   \n",
       "\n",
       "                                                    Weighted F1 score  \\\n",
       "NGR: (1, 1); V: TfidfVectorizer; CLF: LinearSVC...              80.85   \n",
       "\n",
       "                                                    Micro F1 score  \\\n",
       "NGR: (1, 1); V: TfidfVectorizer; CLF: LinearSVC...           79.75   \n",
       "\n",
       "                                                    Macro F1 score  \n",
       "NGR: (1, 1); V: TfidfVectorizer; CLF: LinearSVC...           70.52  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_metrics_df(ngrams_models_metrics_without_irrelevant)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ... it would be skightly better, but we cannot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maybe predicting 'irrelevant' for every not English tweet will help? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = clf.predict(test_text_cleaned)\n",
    "\n",
    "for i in range(len(test_text)):\n",
    "    language = 'en'\n",
    "    try:\n",
    "        language = detect(test_text[i])\n",
    "    except Exception:\n",
    "        pass\n",
    "    if language != 'en':\n",
    "        pred[i] = 'irrelevant'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentiment_cropped = []\n",
    "for i in range(len(test_sentiment)):\n",
    "    if test_sentiment[i] == 'irrelevant':\n",
    "        test_sentiment_cropped.append('irreleva')\n",
    "    else:\n",
    "        test_sentiment_cropped.append(test_sentiment[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Weighted F1 score</th>\n",
       "      <th>Micro F1 score</th>\n",
       "      <th>Macro F1 score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Detecting language</th>\n",
       "      <td>76.02</td>\n",
       "      <td>76.8</td>\n",
       "      <td>76.02</td>\n",
       "      <td>69.52</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Accuracy  Weighted F1 score  Micro F1 score  \\\n",
       "Detecting language     76.02               76.8           76.02   \n",
       "\n",
       "                    Macro F1 score  \n",
       "Detecting language           69.52  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = {'Detecting language': metrics(pred, test_sentiment_cropped)}\n",
    "create_metrics_df(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## No, not really.\n",
    "### The problem is that the language detection performs poorly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What else was done?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sinononyms_antonyms(word):\n",
    "    synonyms = []\n",
    "    antonyms = []\n",
    "    for syn in wordnet.synsets(word):\n",
    "        for l in syn.lemmas():\n",
    "            synonyms.append(l.name())\n",
    "            if l.antonyms():\n",
    "                antonyms.append(l.antonyms()[0].name())\n",
    "    return synonyms, antonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive = []\n",
    "negative = []\n",
    "\n",
    "positive_seed = ['good', 'awesome', 'useful', 'great', 'love', 'favourite']\n",
    "for word in positive_seed:\n",
    "    pos, neg = sinononyms_antonyms(word)\n",
    "    positive.extend(pos)\n",
    "    negative.extend(neg)\n",
    "\n",
    "negative_seed = ['bad', 'hate', 'useless', 'awful', 'dislike', 'shit', 'terrible', 'suck']\n",
    "for word in negative_seed:\n",
    "    neg, pos = sinononyms_antonyms(word)\n",
    "    positive.extend(pos)\n",
    "    negative.extend(neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_in_train = []\n",
    "negative_in_train = []\n",
    "for i in range(len(train_text)):\n",
    "    tweet = train_text[i].lower()\n",
    "    for pos in positive:\n",
    "        if (pos in tweet) and (train_sentiment[i] == 'positive'):\n",
    "            positive_in_train.append(pos)\n",
    "    for neg in negative:\n",
    "        if (neg in tweet) and (train_sentiment[i] == 'negative'):\n",
    "            negative_in_train.append(neg)\n",
    "positive_in_train = list(dict.fromkeys(positive_in_train))\n",
    "negative_in_train = list(dict.fromkeys(negative_in_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Positive adjectives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['just', 'love', 'great', 'bed', 'useful', 'good', 'amazing', 'eff', 'right', 'well', 'like', 'know', 'nice', 'awesome', 'keen', 'sound', 'dear', 'big', 'bully', 'neat', 'near', 'full', 'serious', 'enjoy', 'secure', 'fuck', 'favorite']\n"
     ]
    }
   ],
   "source": [
    "print(positive_in_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Negative adjectives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ill', 'evil', 'make', 'shit', 'suck', 'jack', 'hoot', 'awful', 'painful', 'crap', 'bad', 'hate', 'sorry', 'rat', 'awesome', 'damn', 'terrible', 'useless', 'sucking', 'dire', 'bastard', 'bullshit', 'bull', 'dump', 'stag', 'big']\n"
     ]
    }
   ],
   "source": [
    "print(negative_in_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = []\n",
    "for tweet in test_text_without_irrelevant:\n",
    "    n = 0\n",
    "    p = 0\n",
    "    sentiment = 'neutral'\n",
    "    for pos in positive_in_train:\n",
    "        if pos in tweet:\n",
    "            p +=1 \n",
    "    for neg in negative_in_train:\n",
    "        if neg in tweet:\n",
    "            n +=1\n",
    "    if (p>0) or (n>0):\n",
    "        if p>n:\n",
    "            sentiment = 'positive'\n",
    "        else: \n",
    "            sentiment = 'negative'\n",
    "    pred.append(sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Weighted F1 score</th>\n",
       "      <th>Micro F1 score</th>\n",
       "      <th>Macro F1 score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Playing with adjectives</th>\n",
       "      <td>57.81</td>\n",
       "      <td>59.31</td>\n",
       "      <td>57.81</td>\n",
       "      <td>36.92</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Accuracy  Weighted F1 score  Micro F1 score  \\\n",
       "Playing with adjectives     57.81              59.31           57.81   \n",
       "\n",
       "                         Macro F1 score  \n",
       "Playing with adjectives           36.92  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = {'Playing with adjectives': metrics(pred, test_sentiment_without_irrelevant)}\n",
    "create_metrics_df(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The results are by no means satisfying, but maybe this approach can be used in a more sofisticated manner, maybe together with some other baseline model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the best model for sentiment calssification is **NGR: (1, 2); V: CountVectorizer; CLF: LogisticRegression; MF: 4000**.\n",
    "It achieved the  following results:\n",
    "* **Accuracy**: 78.95\n",
    "* **Weightes F1 score**: 80.03\n",
    "* **Micro F1 score**: 78.95 \n",
    "* **Macro F1 score**: 69.592"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PRODUCT CLASSIFICATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing is slightly different since we don't want to remove mentions and hashtexts which often contain product names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_product(texts):\n",
    "    documents = []\n",
    "    pr.set_options(pr.OPT.URL)\n",
    "    stemmer = WordNetLemmatizer()\n",
    "\n",
    "    for sen in range(0, len(texts)):\n",
    "        # remove URLs, hashtags and mentions\n",
    "        document = pr.clean(texts[sen])\n",
    "\n",
    "        # Converting to Lowercase\n",
    "        document = document.lower()\n",
    "\n",
    "        # Lemmatization\n",
    "        document = document.split()\n",
    "\n",
    "        document = [stemmer.lemmatize(word) for word in document]\n",
    "        document = ' '.join(document)\n",
    "\n",
    "        documents.append(document)\n",
    "        \n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text_product = preprocess_product(train_text)\n",
    "test_text_product = preprocess_product(test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "product_metrics = {}\n",
    "ngram_ranges = [(1, 1), (1, 2), (2, 2)]\n",
    "max_features = [4000, 5000, 6000, 10000, 15000]\n",
    "\n",
    "for ngram_range in ngram_ranges:\n",
    "    for vct_name, vectorizer in zip(vectorizers_names, vectorizers):\n",
    "        for clf_name, classifier in zip(classifiers_names, classifiers):\n",
    "            for mf in max_features:\n",
    "                clf = Pipeline([\n",
    "                    ('vectorizer', vectorizer(analyzer=\"word\",\n",
    "                                                   ngram_range=ngram_range,\n",
    "                                                   tokenizer=word_tokenize,\n",
    "                                                   max_features=mf  ) ),\n",
    "                    ('classifier', classifier)\n",
    "                ])\n",
    "\n",
    "\n",
    "                clf.fit(train_text_product, train_topics)\n",
    "                pred =clf.predict(test_text_product)\n",
    "\n",
    "                name = 'NGR: ' + str(ngram_range) + '; V: ' + vct_name + '; CLF: ' + clf_name + '; MF: ' + str(mf) \n",
    "                product_metrics[name] = metrics(pred, test_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Weighted F1 score</th>\n",
       "      <th>Micro F1 score</th>\n",
       "      <th>Macro F1 score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>NGR: (1, 2); V: CountVectorizer; CLF: LogisticRegression; MF: 15000</th>\n",
       "      <td>86.26</td>\n",
       "      <td>86.26</td>\n",
       "      <td>86.26</td>\n",
       "      <td>85.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NGR: (1, 1); V: CountVectorizer; CLF: LinearSVC; MF: 10000</th>\n",
       "      <td>86.26</td>\n",
       "      <td>86.36</td>\n",
       "      <td>86.26</td>\n",
       "      <td>85.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NGR: (1, 1); V: CountVectorizer; CLF: LinearSVC; MF: 15000</th>\n",
       "      <td>86.26</td>\n",
       "      <td>86.36</td>\n",
       "      <td>86.26</td>\n",
       "      <td>85.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NGR: (1, 2); V: TfidfVectorizer; CLF: LinearSVC; MF: 10000</th>\n",
       "      <td>86.26</td>\n",
       "      <td>86.29</td>\n",
       "      <td>86.26</td>\n",
       "      <td>85.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NGR: (1, 2); V: CountVectorizer; CLF: LogisticRegression; MF: 10000</th>\n",
       "      <td>85.96</td>\n",
       "      <td>85.92</td>\n",
       "      <td>85.96</td>\n",
       "      <td>85.50</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    Accuracy  \\\n",
       "NGR: (1, 2); V: CountVectorizer; CLF: LogisticR...     86.26   \n",
       "NGR: (1, 1); V: CountVectorizer; CLF: LinearSVC...     86.26   \n",
       "NGR: (1, 1); V: CountVectorizer; CLF: LinearSVC...     86.26   \n",
       "NGR: (1, 2); V: TfidfVectorizer; CLF: LinearSVC...     86.26   \n",
       "NGR: (1, 2); V: CountVectorizer; CLF: LogisticR...     85.96   \n",
       "\n",
       "                                                    Weighted F1 score  \\\n",
       "NGR: (1, 2); V: CountVectorizer; CLF: LogisticR...              86.26   \n",
       "NGR: (1, 1); V: CountVectorizer; CLF: LinearSVC...              86.36   \n",
       "NGR: (1, 1); V: CountVectorizer; CLF: LinearSVC...              86.36   \n",
       "NGR: (1, 2); V: TfidfVectorizer; CLF: LinearSVC...              86.29   \n",
       "NGR: (1, 2); V: CountVectorizer; CLF: LogisticR...              85.92   \n",
       "\n",
       "                                                    Micro F1 score  \\\n",
       "NGR: (1, 2); V: CountVectorizer; CLF: LogisticR...           86.26   \n",
       "NGR: (1, 1); V: CountVectorizer; CLF: LinearSVC...           86.26   \n",
       "NGR: (1, 1); V: CountVectorizer; CLF: LinearSVC...           86.26   \n",
       "NGR: (1, 2); V: TfidfVectorizer; CLF: LinearSVC...           86.26   \n",
       "NGR: (1, 2); V: CountVectorizer; CLF: LogisticR...           85.96   \n",
       "\n",
       "                                                    Macro F1 score  \n",
       "NGR: (1, 2); V: CountVectorizer; CLF: LogisticR...           85.77  \n",
       "NGR: (1, 1); V: CountVectorizer; CLF: LinearSVC...           85.63  \n",
       "NGR: (1, 1); V: CountVectorizer; CLF: LinearSVC...           85.63  \n",
       "NGR: (1, 2); V: TfidfVectorizer; CLF: LinearSVC...           85.74  \n",
       "NGR: (1, 2); V: CountVectorizer; CLF: LogisticR...           85.50  "
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_metrics_df(product_metrics).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the best model for product calssification is **NGR: (1, 2); V: CountVectorizer; CLF: LogisticRegression; MF: 15000**.\n",
    "It achieved the following results:\n",
    "* **Accuracy**: 86.26\n",
    "* **Weightes F1 score**: 86.26\n",
    "* **Micro F1 score**: 86.26\n",
    "* **Macro F1 score**: 85.63"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
